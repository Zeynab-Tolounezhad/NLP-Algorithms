# -*- coding: utf-8 -*-
"""NLP_Probabilistic Language Modeling_N-gram.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dz9hB9TGJigQrvcU-EgnRSu2mG9DazjK

# Probabilistic Language Modeling
1.   **Algorithm** (N-gram)
2.   **Library** (NLTK)
3.   **Result** (Word prediction using n-gram model)
"""

import io
import re
from nltk import FreqDist
from nltk.util import ngrams
import collections

"""Downlaod data"""

!gdown --id 1Hb58rR--Qjwr21cLK6A29ROp6Uy5pqq8

"""Preprocess

Ignoring
*   extra consecutive spaces
*   spaces at the beginning and end of lines


"""

inputFile = io.open('train.txt', mode="r", encoding="utf-8")
outputFile = io.open('cleaned_train.txt', mode="w", encoding="utf-8")

updated=[]
Lines = inputFile.readlines()
for i, line in enumerate(Lines):
    newline= re.sub(' +', ' ', line.strip())
    if len(line.split()) >2:
        updated.append(newline+"\n")
    else:
        print(line,i)
outputFile.writelines(updated)

print( len(Lines), len(updated))
inputFile.__exit__()
outputFile.__exit__()

"""**Q 1:**
>Counting unigram and bigram combinations

"""

def count_unigrams(inputfile):
    textfile = io.open(inputfile, mode="r", encoding="utf-8")
    Lines = textfile.readlines()
    textfile.__exit__()
    counts = dict()
    for line in Lines:
        tokens = line.strip().split(' ')
        if len(tokens)>0:
            for word in tokens:
                if word in counts:
                    counts[word] += 1
                else:
                    counts[word] = 1
    return counts
unigrams_dict=count_unigrams('cleaned_train.txt')
unigrams_sorted = list(sorted(unigrams_dict.items(), key=lambda item: item[1],reverse=True))

for i,tuple_ in enumerate(unigrams_sorted[:20]):
    rank_=i+1
    item, count=tuple_
    text = "{}\t{}\t{}\t{}{}".format(rank_, count, count*rank_, item, "\n")
    print(text)

def count_bigrams(inputfile):
    textfile = io.open(inputfile, mode="r", encoding="utf-8")
    Lines = textfile.readlines()
    textfile.__exit__()
    counts=collections.Counter()
    for line in Lines:
        tokens = line.strip().split(' ')


        if len(tokens)>1:

            for tuple_ in zip(tokens[:-1],tokens[1:]):
                if tuple_ in counts:
                    counts[tuple_] += 1
                else:
                    counts[tuple_] = 1

    return counts
bigrams_dict=count_bigrams('cleaned_train.txt')
bigrams_sorted = list(sorted(bigrams_dict.items(), key=lambda item: item[1],reverse=True))


for i,tuple_ in enumerate(bigrams_sorted[:20]):
    rank_=i+1
    item, count=tuple_
    text = "{}\t{}\t{}\t{}{}".format(rank_, count, count*rank_, " ".join(item), "\n")

    print(text)

"""**Q 2:**

> Writing a function for N-gram


"""

def compute_freq(inputfile, nu):
    textfile = io.open(inputfile, mode="r", encoding="utf-8")
    Lines = textfile.readlines()
    textfile.__exit__()
    ngramfdist = FreqDist()

    for i,line in enumerate(Lines):
        tokens = line.strip().split(' ')
        if len(tokens)>nu:
            ngram_ = ngrams(tokens, nu)
            ngramfdist.update(ngram_)
    return ngramfdist

esBigramFreq = collections.Counter(compute_freq("cleaned_train.txt",5))

for i,tuple_ in enumerate(esBigramFreq.most_common(20)):
    rank_=i+1
    item, count=tuple_
    text = "{}\t{}".format(count, " ".join(item), "\n")
    print(text)

"""**Q 3:**

> Calculate the probability of the specified expressions


"""

sentences_to_calculate=["چون تویی آید به زیبایی و شیرینی پسر",
                        "دل در این درد و رنج پاره کنیم",
                        "ای به آرام تو زمین را سنگ",
                        "جان را زند آ باغ صلاهای تعالوا",
                        "شاهد و شمع و شراب و مطرب آنجا بهترست",
                        "شب است و شمع و شراب و شیرینی"]

"""uingram_prob"""

unigram_sum = sum([val for key, val in unigrams_sorted])
for sentence in sentences_to_calculate:
    tokens = sentence.strip().split(' ')
    prob=1
    for token in tokens:
        count = unigrams_dict[token]
        prob = prob * (count*1.00/unigram_sum)
    print(prob,"\t",sentence)
print()

"""bigram_prob"""

for sentence in sentences_to_calculate:
    tokens = sentence.strip().split(' ')
    prob= (unigrams_dict[tokens[0]]*1.00)/unigram_sum
    for w1,w2 in zip(tokens[:-1],tokens[1:]):
        prob = prob * bigrams_dict[(w1,w2)] / unigrams_dict[w1]
    print(prob,"\t",sentence)
print()

"""trigram_prob"""

trigram_Freq = collections.Counter(compute_freq("cleaned_train.txt",3))

for sentence in sentences_to_calculate:
    tokens = sentence.strip().split(' ')
    prob=  (unigrams_dict[tokens[0]]*1.00)/unigram_sum * bigrams_dict[(tokens[0],tokens[1])] / unigrams_dict[tokens[0]]
    for w1,w2,w3 in zip(tokens[:-2],tokens[1:-1], tokens[2:]):
        prob = prob * trigram_Freq[(w1,w2,w3)] / bigrams_dict[(w1,w2)]
    print(prob,"\t",sentence)

"""**Q 4. Why is the probability of this sentence zero?** (Solution: smoothing methods)

> شب است و شمع و شراب و شیرینی

"""

for w1,w2,w3 in zip(tokens[:-2], tokens[1:-1], tokens[2:]):
  print(w1, w2, w3)
  print(trigram_Freq[(w1, w2, w3)])

"""**Q 5**

> Complete the following expressions using the bigram model



1.   چون مشک سیه بود مرا هر دو بنا
2.   گر خورد سوگند هم آن
3.   زانک نفس آشفته تر گردد از
4.   ازین زشت تر در جهان رنگ




"""

incomplete_sentences = [
                       "چون مشک سیه بود مرا هر دو بنا",
                       "گر خورد سوگند هم آن",
                       "زانک نفس آشفته تر گردد از",
                       "ازین زشت تر در جهان رنگ ",
                       "شب است و شمع و شراب و",
]
for sentence in incomplete_sentences:
    tokens = sentence.strip().split(' ')
    last_word = tokens [-1]

    possible_options = {w2:count for (w1,w2),count in bigrams_dict.items() if w1==last_word}
    options_list=sorted(possible_options.items(), key=lambda item: item[1],reverse=True)
    print(sentence)
    print(options_list)
    print(sentence,options_list[0][0])
    print(sentence,options_list[1][0])
    print()